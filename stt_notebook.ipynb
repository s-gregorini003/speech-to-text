{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "stt_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fljBsBFHWCMi"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s-gregorini003/speech-to-text/blob/main/stt_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhBo4UlPaq4O"
      },
      "source": [
        "# Speech Recognition with Python\n",
        "\n",
        "There are several Automated Speech Recognition (ASR) alternatives, and most of them have bindings for Python. There are two kinds of solutions:\n",
        "\n",
        "- **Service:** These run on the cloud, and are accessed either through REST endpoints or Python library. Examples are cloud speech services from Google, Amazon, Microsoft.\n",
        "- **Software:** These run locally on the machine (not requiring network connection). Examples are CMU Sphinx and Mozilla DeepSpeech.\n",
        "\n",
        "Speech Recognition APIs are of two types:\n",
        "- **Batch:** The full audio file is passed as parameter, and speech-to-text transcribing is done in one shot.\n",
        "- **Streaming:** The chunks of audio buffer are repeatedly passed on, and intermediate results are accessible.\n",
        "\n",
        "All packages support batch mode, and some support streaming mode too.\n",
        "\n",
        "One common use case is to collect audio from microphone and passes on the buffer to the speech recognition API. Invariably, in such transcribers, microphone is accessed though [PyAudio](https://people.csail.mit.edu/hubert/pyaudio/), which is implemented over [PortAudio](http://www.portaudio.com/).\n",
        "\n",
        "From Colab menu, select: **Runtime** > **Change runtime type**, and verify that it is set to Python3, and select GPU if you want to try out GPU version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H6HFpi_LCBt"
      },
      "source": [
        "## Common Setup\n",
        "\n",
        "1. **Install google cloud speech package**\n",
        "\n",
        "You may have to restart the runtime after this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhIMRXQPLaaA",
        "outputId": "005c6b49-2fcf-4021-ec5c-b8d01ae392c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install google-cloud-speech\n",
        "!pip install pydub"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting google-cloud-speech\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/2e/e94efd29e17f239f84d3774cd15e9135ddb2601206ac56ac4d0ebe81cfef/google_cloud_speech-2.0.0-py2.py3-none-any.whl (108kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 10.4MB/s \n",
            "\u001b[?25hCollecting libcst>=0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/73/dc3f4b0ec439212bfd2dab67b44ba86966e69e073bd8e93057dab127ba5e/libcst-0.3.13-py3-none-any.whl (502kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 21.8MB/s \n",
            "\u001b[?25hCollecting proto-plus>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/0d/dd8e8c979eb843b873aeaadcfefd5383da7e2fc5593456a32264cae1cc5c/proto-plus-1.11.0.tar.gz (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.9MB/s \n",
            "\u001b[?25hCollecting google-api-core[grpc]<2.0.0dev,>=1.22.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/3d/d7af13040ab5b259994a4434ff03d68084a994e709bc8afa4bee1235310e/google_api_core-1.23.0-py2.py3-none-any.whl (91kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.8MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 42.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses>=0.6.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from libcst>=0.2.5->google-cloud-speech) (0.7)\n",
            "Collecting typing-inspect>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/42/1c/66402db44184904a2f14722d317a4da0b5c8c78acfc3faf74362566635c5/typing_inspect-0.6.0-py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.2 in /usr/local/lib/python3.6/dist-packages (from libcst>=0.2.5->google-cloud-speech) (3.7.4.3)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.6/dist-packages (from proto-plus>=1.4.0->google-cloud-speech) (3.12.4)\n",
            "Collecting google-auth<2.0dev,>=1.21.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/60/81e68e70eea91ef05bb00bcdac243d67b61f826c65aaca6961de622dffd7/google_auth-1.23.0-py2.py3-none-any.whl (114kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 51.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-speech) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-speech) (50.3.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-speech) (1.15.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-speech) (2018.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-speech) (1.52.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.29.0; extra == \"grpc\" in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-speech) (1.33.2)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-speech) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-speech) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-speech) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-speech) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-speech) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-speech) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-speech) (2.10)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-speech) (0.4.8)\n",
            "Building wheels for collected packages: proto-plus, pyyaml\n",
            "  Building wheel for proto-plus (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for proto-plus: filename=proto_plus-1.11.0-cp36-none-any.whl size=41571 sha256=86da267d4a6e15ea738542bd87fe44d4c331077b88a3120a63ead9fff6fc3b59\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/e3/6b/a14506581b1cde1ac1743f2939dcc06fc06a5af2aa224a334e\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44619 sha256=a1ce3f7e56551439a9167a87291f8530631a7acb6169446093344aae592d6a98\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built proto-plus pyyaml\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.17.2, but you'll have google-auth 1.23.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pyyaml, mypy-extensions, typing-inspect, libcst, proto-plus, google-auth, google-api-core, google-cloud-speech\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: google-auth 1.17.2\n",
            "    Uninstalling google-auth-1.17.2:\n",
            "      Successfully uninstalled google-auth-1.17.2\n",
            "  Found existing installation: google-api-core 1.16.0\n",
            "    Uninstalling google-api-core-1.16.0:\n",
            "      Successfully uninstalled google-api-core-1.16.0\n",
            "Successfully installed google-api-core-1.23.0 google-auth-1.23.0 google-cloud-speech-2.0.0 libcst-0.3.13 mypy-extensions-0.4.3 proto-plus-1.11.0 pyyaml-5.3.1 typing-inspect-0.6.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting pydub\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/d1/fbfa79371a8cd9bb15c2e3c480d7e6e340ed5cc55005174e16f48418333a/pydub-0.24.1-py2.py3-none-any.whl\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.24.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUjuKePpRJt5"
      },
      "source": [
        "2. **Download audio files for testing**\n",
        "\n",
        "Following files will be used as test cases for all speech recognition alternatives covered in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gio-HCqWeVaq",
        "outputId": "418fb7e9-8619-474b-f933-98d063d75896",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 602
        }
      },
      "source": [
        "# upload mp3 audio file.\n",
        "import os\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for audio in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "         name=audio, length=len(uploaded[audio])))\n",
        "\n",
        "os.rename(audio, 'speech.mp3')\n",
        "audio = 'speech.wav'\n",
        "\n",
        "# convert to wav file.  \n",
        "!ffmpeg -i speech.mp3 -vn -acodec pcm_s16le -ac 1 -ar 16000 -f wav speech.wav\n",
        "#!mpg123 -w speech.wav speech.mp3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dfbd9e2c-fb7b-4757-8076-1653757fbfe2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-dfbd9e2c-fb7b-4757-8076-1653757fbfe2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving AUDIO_Ann's Proximal Screening.mp3 to AUDIO_Ann's Proximal Screening.mp3\n",
            "User uploaded file \"AUDIO_Ann's Proximal Screening.mp3\" with length 101102644 bytes\n",
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, mp3, from 'speech.mp3':\n",
            "  Duration: 01:23:40.26, start: 0.025057, bitrate: 161 kb/s\n",
            "    Stream #0:0: Audio: mp3, 44100 Hz, stereo, s16p, 161 kb/s\n",
            "    Metadata:\n",
            "      encoder         : LAME3.100\n",
            "File 'speech.wav' already exists. Overwrite ? [y/N] y\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (mp3 (native) -> pcm_s16le (native))\n",
            "Press [q] to stop, [?] for help\n",
            "Output #0, wav, to 'speech.wav':\n",
            "  Metadata:\n",
            "    ISFT            : Lavf57.83.100\n",
            "    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/s\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 pcm_s16le\n",
            "size=  156882kB time=01:23:40.22 bitrate= 256.0kbits/s speed= 431x    \n",
            "video:0kB audio:156882kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000049%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJKXKnOExR49",
        "outputId": "9696f79f-0708-4dd2-e464-d5581e8229c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "!mkdir audio/\n",
        "\n",
        "newAudio = AudioSegment.from_wav(\"speech.wav\")\n",
        "chunks = len(newAudio) // 300000\n",
        "\n",
        "for c in range(chunks):\n",
        "  if c < 10:\n",
        "    tmpAudio = newAudio[c*300000:c*300000+300000]\n",
        "    tmpAudio.export('audio/0{0}.wav'.format(c), format=\"wav\") #Exports to a wav file in the current path.\n",
        "  else:\n",
        "    tmpAudio = newAudio[c*300000:c*300000+300000]\n",
        "    tmpAudio.export('audio/{0}.wav'.format(c), format=\"wav\") #Exports to a wav file in the current path.\n",
        "\n",
        "tmpAudio = newAudio[chunks*300000:]\n",
        "tmpAudio.export('audio/{0}.wav'.format(chunks), format=\"wav\") #Exports to a wav file in the current path."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_io.BufferedRandom name='audio/16.wav'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXzH4pu9Kxr4"
      },
      "source": [
        "3. **Define test cases**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbzYo01kRi8P",
        "outputId": "b6bcfcd4-cfeb-4f93-ae04-9b7491c23816",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lst = os.listdir(\"audio/\")\n",
        "\n",
        "chunks_config = []\n",
        "\n",
        "for i in sorted(lst):\n",
        "  chunks_config.append({ 'filename': \"audio/{0}\".format(i),'encoding': 'LINEAR16','lang': 'en-US' })\n",
        "\n",
        "print(chunks_config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'filename': 'audio/00.wav', 'encoding': 'LINEAR16', 'lang': 'en-US'}, {'filename': 'audio/01.wav', 'encoding': 'LINEAR16', 'lang': 'en-US'}, {'filename': 'audio/02.wav', 'encoding': 'LINEAR16', 'lang': 'en-US'}, {'filename': 'audio/03.wav', 'encoding': 'LINEAR16', 'lang': 'en-US'}, {'filename': 'audio/04.wav', 'encoding': 'LINEAR16', 'lang': 'en-US'}, {'filename': 'audio/05.wav', 'encoding': 'LINEAR16', 'lang': 'en-US'}, {'filename': 'audio/06.wav', 'encoding': 'LINEAR16', 'lang': 'en-US'}, {'filename': 'audio/07.wav', 'encoding': 'LINEAR16', 'lang': 'en-US'}, {'filename': 'audio/08.wav', 'encoding': 'LINEAR16', 'lang': 'en-US'}, {'filename': 'audio/09.wav', 'encoding': 'LINEAR16', 'lang': 'en-US'}, {'filename': 'audio/10.wav', 'encoding': 'LINEAR16', 'lang': 'en-US'}, {'filename': 'audio/11.wav', 'encoding': 'LINEAR16', 'lang': 'en-US'}, {'filename': 'audio/12.wav', 'encoding': 'LINEAR16', 'lang': 'en-US'}, {'filename': 'audio/13.wav', 'encoding': 'LINEAR16', 'lang': 'en-US'}, {'filename': 'audio/14.wav', 'encoding': 'LINEAR16', 'lang': 'en-US'}, {'filename': 'audio/15.wav', 'encoding': 'LINEAR16', 'lang': 'en-US'}, {'filename': 'audio/16.wav', 'encoding': 'LINEAR16', 'lang': 'en-US'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-23iCNLvBIx"
      },
      "source": [
        "4. **Utility Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujeuvj35Ksv8"
      },
      "source": [
        "from typing import Tuple\n",
        "import wave\n",
        "\n",
        "def read_wav_file(filename) -> Tuple[bytes, int]:\n",
        "    with wave.open(filename, 'rb') as w:\n",
        "        rate = w.getframerate()\n",
        "        frames = w.getnframes()\n",
        "        buffer = w.readframes(frames)\n",
        "\n",
        "    return buffer, rate\n",
        "\n",
        "def simulate_stream(buffer: bytes, batch_size: int = 4096):\n",
        "    buffer_len = len(buffer)\n",
        "    offset = 0\n",
        "    while offset < buffer_len:\n",
        "        end_offset = offset + batch_size\n",
        "        buf = buffer[offset:end_offset]\n",
        "        yield buf\n",
        "        offset = end_offset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wFdQoEUQH-3"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Google\n",
        "\n",
        "Google has [speech-to-text](https://cloud.google.com/speech-to-text/docs) as one of the Google Cloud services. It has [libraries](https://cloud.google.com/speech-to-text/docs/reference/libraries) in C#, Go, Java, JavaScript, PHP, Python, and Ruby. It supports both batch and stream modes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjcnbQvvY3Xu"
      },
      "source": [
        "## Setup\n",
        "\n",
        "1. **Upload Google Cloud Cred file**\n",
        "\n",
        "Have Google Cloud creds stored in a file named **`gc-creds.json`**, and upload it by running following code cell. See https://developers.google.com/accounts/docs/application-default-credentials for more details.\n",
        "\n",
        "This may reqire enabling **third-party cookies**. Check out https://colab.research.google.com/notebooks/io.ipynb for other alternatives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXquL3Y7bLQ6",
        "outputId": "97c71195-9c6f-4884-ebd6-308fde7ff786",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4d496949-36f4-4eed-954d-e35c748ad736\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4d496949-36f4-4eed-954d-e35c748ad736\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving gc-creds.json to gc-creds.json\n",
            "User uploaded file \"gc-creds.json\" with length 2334 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-emXbdQ1bTDg",
        "outputId": "2d6bb323-f5ea-4d0e-d9ec-e1d744b64052",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pwd\n",
        "!ls -l ./gc-creds.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "-rw-r--r-- 1 root root 2334 Nov  8 12:38 ./gc-creds.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-80JnSyMWEV"
      },
      "source": [
        "2. **Set environment variable**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msFTMyUWgtEv",
        "outputId": "55c84a6f-02af-4d40-dfae-a50bfcc0c545",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/gc-creds.json'\n",
        "\n",
        "!ls -l $GOOGLE_APPLICATION_CREDENTIALS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 2334 Nov  8 12:38 /content/gc-creds.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fljBsBFHWCMi"
      },
      "source": [
        "## Batch API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dlm4CWyQPeR",
        "outputId": "cdc849c7-4d2c-4848-969c-6e0552c9eec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "from google.cloud import speech_v1\n",
        "# from google.cloud.speech_v1 import enums\n",
        "\n",
        "def google_batch_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    buffer, rate = read_wav_file(filename)\n",
        "    client = speech_v1.SpeechClient()\n",
        "\n",
        "    # config = {\n",
        "    #     'language_code': lang,\n",
        "    #     'sample_rate_hertz': rate,\n",
        "    #     'encoding': speech_v1.RecognitionConfig.AudioEncoding[encoding]\n",
        "    # }\n",
        "\n",
        "    audio = {\n",
        "        'content': buffer\n",
        "    }\n",
        "\n",
        "    config = speech_v1.RecognitionConfig(\n",
        "        encoding=speech_v1.RecognitionConfig.AudioEncoding[encoding],\n",
        "        sample_rate_hertz=16000,\n",
        "        language_code=\"en-US\",\n",
        "    )\n",
        "\n",
        "    operation = client.long_running_recognize(\n",
        "        request={\"config\": config, \"audio\": audio}\n",
        "    )\n",
        "\n",
        "    operation = client.long_running_recognize(config=config, audio=audio)\n",
        "    # # response = client.recognize(config, audio)\n",
        "    # # For bigger audio file, the previous line can be replaced with following:\n",
        "    # operation = client.long_running_recognize(config, audio)\n",
        "    response = operation.result()\n",
        "\n",
        "    for result in response.results:\n",
        "        # First alternative is the most probable result\n",
        "        alternative = result.alternatives[0]\n",
        "        return alternative.transcript\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    # print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\n",
        "    print('google-cloud-batch-stt: \"{}\"'.format(\n",
        "        google_batch_stt(t['filename'], t['lang'], t['encoding'])\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgument",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    922\u001b[0m                                       wait_for_ready, compression)\n\u001b[0;32m--> 923\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"Request payload size exceeds the limit: 10485760 bytes.\"\n\tdebug_error_string = \"{\"created\":\"@1604678108.360283145\",\"description\":\"Error received from peer ipv4:172.217.9.202:443\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1061,\"grpc_message\":\"Request payload size exceeds the limit: 10485760 bytes.\",\"grpc_status\":3}\"\n>",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-2e72e84e8874>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     print('google-cloud-batch-stt: \"{}\"'.format(\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mgoogle_batch_stt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lang'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     ))\n",
            "\u001b[0;32m<ipython-input-25-2e72e84e8874>\u001b[0m in \u001b[0;36mgoogle_batch_stt\u001b[0;34m(filename, lang, encoding)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     operation = client.long_running_recognize(\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"audio\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     )\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/cloud/speech_v1/services/speech/client.py\u001b[0m in \u001b[0;36mlong_running_recognize\u001b[0;34m(self, request, config, audio, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;31m# Wrap the response in an operation future.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgument\u001b[0m: 400 Request payload size exceeds the limit: 10485760 bytes."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGhaFWC7rN9b"
      },
      "source": [
        "## Streaming API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9wMydkzrdX-",
        "outputId": "e3fa7b15-5b0e-4f26-ab5a-7b2e8322a254",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.cloud import speech_v1\n",
        "\n",
        "def response_stream_processor(responses):\n",
        "    print('interim results: ')\n",
        "\n",
        "    transcript = ''\n",
        "    num_chars_printed = 0\n",
        "    for response in responses:\n",
        "        if not response.results:\n",
        "            continue\n",
        "\n",
        "        result = response.results[0]\n",
        "        if not result.alternatives:\n",
        "            continue\n",
        "\n",
        "        transcript = result.alternatives[0].transcript\n",
        "\n",
        "        if result.is_final:\n",
        "          f = open(\"transcript.txt\", 'a')\n",
        "          f.write(transcript)\n",
        "\n",
        "        # print('{0}final: {1}'.format(\n",
        "        #     '' if result.is_final else 'not ',\n",
        "        #     transcript\n",
        "        # ))\n",
        "\n",
        "    return transcript\n",
        "\n",
        "def google_streaming_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    buffer, rate = read_wav_file(filename)\n",
        "\n",
        "    client = speech_v1.SpeechClient()\n",
        "\n",
        "    config = speech_v1.RecognitionConfig(\n",
        "        encoding=speech_v1.RecognitionConfig.AudioEncoding[encoding],\n",
        "        sample_rate_hertz=rate,\n",
        "        language_code=lang\n",
        "    )\n",
        "\n",
        "    streaming_config = speech_v1.StreamingRecognitionConfig(\n",
        "        config=config,\n",
        "        interim_results=True\n",
        "    )\n",
        "\n",
        "    audio_generator = simulate_stream(buffer)  # buffer chunk generator\n",
        "    requests = (speech_v1.StreamingRecognizeRequest(audio_content=chunk) for chunk in audio_generator)\n",
        "    responses = client.streaming_recognize(streaming_config, requests)\n",
        "    # Now, put the transcription responses to use.\n",
        "    return response_stream_processor(responses)\n",
        "\n",
        "# Run tests\n",
        "for t in chunks_config:\n",
        "    print('\\naudio file=\"{0}\"'.format(t['filename']))\n",
        "    print('google-cloud-streaming-stt: \"{}\"'.format(\n",
        "        google_streaming_stt(t['filename'], t['lang'], t['encoding'])\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "audio file=\"audio/00.wav\"\n",
            "interim results: \n",
            "google-cloud-streaming-stt: \" we think about the term double Crush cell what is there's a triple Crush what if there's three areas involved maybe a cervical radiculopathy a problem at the thoracic outlet and a carpal tunnel issue all of these things have discussed compression but what about other mechanical stresses on the nurse traction\"\n",
            "\n",
            "audio file=\"audio/01.wav\"\n",
            "interim results: \n",
            "google-cloud-streaming-stt: \" so this is a study that you can vote who is also one of the Iowa instructors did looking at the relationship of thoracic outlet and carpal tunnel and we're going to talk a lot more about the car at the thoracic outlet Shoppes a little bit later so what she looked at it was 32 subjects with EMG tons of carpal tunnel and 32 HVAC controls and the exam included the elevated on stressed out and we're going to talk about that bass the roost pass that we are familiar with a test called the seereax released test and we're going to be discussing that as part of the thoracic outlet section but it's his past where you unload the brachial plexus by passively elevating the scapula and you're looking for reproduction of symptoms so if the test is positive typically it's going to reproduce their semi-final Chaska cervical rotation lateral flexion test is a testis evaluating the mobility the first rib cage and where can\"\n",
            "\n",
            "audio file=\"audio/02.wav\"\n",
            "interim results: \n",
            "google-cloud-streaming-stt: \" when we look at what is the clinical presentation of an internal disruption so we have inflammatory non-inflammatory chemical Cascade happening\"\n",
            "\n",
            "audio file=\"audio/03.wav\"\n",
            "interim results: \n",
            "google-cloud-streaming-stt: \" when we're listening for a patient's history radicular pain is going to be our most common clinical presentation followed by sensory disturbances so pain is going to be the most common weakness is actually going to occur only 15% of your population so that's going to not be quite as common as pain and when do people come in to see us when ibuprofen or other anti-inflammatory stops working that's when people come in so much pain is going to be the number one reason\"\n",
            "\n",
            "audio file=\"audio/04.wav\"\n",
            "interim results: \n",
            "google-cloud-streaming-stt: \" as far as for our spurling's test Solantic Stein and Associates did a study to look at what's the best way to do the spurling's test since it was introduced there's like six different ways to do it and there's combinations of extension side bending rotation and axial compression and what they found the most efficient and best way to do it are to do first at 1 where you're going to extend and side then and if their test is positive you stop there okay if it's not that's when we're going to add the axial compression and if she was positive that indicates in a vertebral foramen compression or a radiculopathy and I like using this and stuff because it has asked me if I've seen a patient with cervical radiculopathy if I see them on their first visit and the first step elicits their symptoms and I'm doing a\"\n",
            "\n",
            "audio file=\"audio/05.wav\"\n",
            "interim results: \n",
            "google-cloud-streaming-stt: \" so that systematic review that shrooms and Associates it in 2018 that said it was regards to evaluation of dermatomes myotomes and reflexes that no studies were found with Sensodyne on accuracy so we don't want to use these and isolation I still think that they can be valuable to help us to discern what level\"\n",
            "\n",
            "audio file=\"audio/06.wav\"\n",
            "interim results: \n",
            "google-cloud-streaming-stt: \" so to summarize what we find with her cervical clinical presentation with cervical flexion if the sentence increase it more likely due to a disc so we're loading at disc the pain remember it's going to be more vague cash unless the discs pushing post your letter on the nerve that's when we'll see radicular symptoms but keep in mind a classic disc issue is going to be more vague symptoms when we look at extension so now we're closing out that intervertebral foramen we're going to cause nerve root irritation so we're spurling's test we're looking for again going into extension side bending looking for provocation if there's nothing that we can add the Ab Circle Pro\"\n",
            "\n",
            "audio file=\"audio/07.wav\"\n",
            "interim results: \n",
            "google-cloud-streaming-stt: \" this is a fascinating article that I think it's important for us to keep in mind so this group they looked at page\"\n",
            "\n",
            "audio file=\"audio/08.wav\"\n",
            "interim results: \n",
            "google-cloud-streaming-stt: \" cos you know one was a volleyball player another one was really into yoga doing that a significant amount of time a young woman with her arms and overhead position weather here in a downward dog position so again these are a population we typically don't see in the clinic however the venous TOS often times you'll see that in conjunction with something called dispute a neurogenic shock\"\n",
            "\n",
            "audio file=\"audio/09.wav\"\n",
            "interim results: \n",
            "google-cloud-streaming-stt: \" so the most common issue with the speed of neurogenic thoracic outlet is traction so we get a calling retract mean on a nurse so commonly then sees the image James negative their vascular test don't really tell us anything cuz we're affecting the vision or more of those little blood vessels that supply the epineurium they are sensitive to stress so for electrical studies really aren't that helpful but can't refuse are the first sign it because the traction intermittent we're not going to end up getting that permanent axonal death or Exile deficit so there is one exception to this and this is something that's Sanders also published that the vast majority of those patients who fell into that disputed group he's finding some do have objective\"\n",
            "\n",
            "audio file=\"audio/10.wav\"\n",
            "interim results: \n",
            "google-cloud-streaming-stt: \"the blood supply so let's think about what happens and I think women can relate this little more than match what happens when you sit on your foot when your foot is asleep you don't know what to sleep until you bring it out from underneath you and the blood\"\n",
            "\n",
            "audio file=\"audio/11.wav\"\n",
            "interim results: \n",
            "google-cloud-streaming-stt: \"your side you're going to pull up or you're going to go from here alternating and this is I found over the years to be better cuz he's people have big heavy arms typically so you're going to pass Leon way Kevin you're going to hold that for 60 seconds 60 seconds could be a long time so in your watching the clock waiting to see do they have provocation of sentence so the time of day though it's going to matter when you do that so for those of you with smaller arms\"\n",
            "\n",
            "audio file=\"audio/12.wav\"\n",
            "interim results: \n",
            "google-cloud-streaming-stt: \"so that's what we're looking for so for positive test. She has a negative test she is very\"\n",
            "\n",
            "audio file=\"audio/13.wav\"\n",
            "interim results: \n",
            "google-cloud-streaming-stt: \" big things that can play a huge role if you can get their scapula off in a better position big things can happen as far as nerve wise alleviating that tension and improving their neural Mobility we look at to just are Dynamic scapular control and a lot of these people when they reach and Rain they they're scapular more depressed we see some deep creases after AC joint and so working on just getting that elevation can be very helpful for them to start again stop closing off especially the space beneath PEC minor that's the coracoid space so here showing so this is more normal so that we get appropriate upward rotation about 60° with a little bit of elevation now what we see here are those deep creases I was to\"\n",
            "\n",
            "audio file=\"audio/14.wav\"\n",
            "interim results: \n",
            "google-cloud-streaming-stt: \" and then this is a basic question but just what is the normal position you should see the scapula and when you're just at rest and they're standing there looking at it cuz\"\n",
            "\n",
            "audio file=\"audio/15.wav\"\n",
            "interim results: \n",
            "google-cloud-streaming-stt: \" and for the question about the dura that and whether somebody can distinguish whether pain is coming from disc versus Dora and so many cases they're so close together\"\n",
            "\n",
            "audio file=\"audio/16.wav\"\n",
            "interim results: \n",
            "google-cloud-streaming-stt: \" thank you all for joining me today and hopefully this will be helpful in the clinic I know I miss teaching so much this is it's fine I wish that we could be in person but thanks again and hope you have a good rest your day thank you\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8Fg2BE75Qoo"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Microsoft Azure\n",
        "\n",
        "Microsoft Azure [Speech Services](https://azure.microsoft.com/en-in/services/cognitive-services/speech-services/) have [Speech to Text](https://azure.microsoft.com/en-in/services/cognitive-services/speech-to-text/) service."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk8NgzQIlwwX"
      },
      "source": [
        "## Setup\n",
        "\n",
        "1. **Install azure speech package**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A5YJHlswQSs",
        "outputId": "4f2e7f65-dfcc-450e-cb26-c9ca780957c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!pip3 install azure-cognitiveservices-speech"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting azure-cognitiveservices-speech\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/d8/690896a3543b7bed058029b1b3450f4ce2e952d19347663fe570e6dec72c/azure_cognitiveservices_speech-1.9.0-cp36-cp36m-manylinux1_x86_64.whl (3.9MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 3.4MB/s \n",
            "\u001b[?25hInstalling collected packages: azure-cognitiveservices-speech\n",
            "Successfully installed azure-cognitiveservices-speech-1.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4ME2jnAimEQ"
      },
      "source": [
        "2. **Set service credentials**\n",
        "\n",
        "You can enable Speech service and find credentials for your account at [Microsoft Azure portal](https://portal.azure.com/). You can open a free account [here](https://azure.microsoft.com/en-in/free/ai/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSqzFx-lwyz7"
      },
      "source": [
        "AZURE_SPEECH_KEY = 'YOUR AZURE SPEECH KEY'\n",
        "AZURE_SERVICE_REGION = 'YOUR AZURE SERVICE REGION'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVvMt_qylUjF"
      },
      "source": [
        "## Batch API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRMjNB68wYYN",
        "outputId": "7539fb6e-1625-4931-f981-96874628c934",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "import azure.cognitiveservices.speech as speechsdk\n",
        "\n",
        "def azure_batch_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    speech_config = speechsdk.SpeechConfig(\n",
        "        subscription=AZURE_SPEECH_KEY,\n",
        "        region=AZURE_SERVICE_REGION\n",
        "    )\n",
        "    audio_input = speechsdk.AudioConfig(filename=filename)\n",
        "    speech_recognizer = speechsdk.SpeechRecognizer(\n",
        "        speech_config=speech_config,\n",
        "        audio_config=audio_input\n",
        "    )\n",
        "    result = speech_recognizer.recognize_once()\n",
        "\n",
        "    return result.text if result.reason == speechsdk.ResultReason.RecognizedSpeech else None\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\n",
        "    print('azure-batch-stt: \"{}\"'.format(\n",
        "        azure_batch_stt(t['filename'], t['lang'], t['encoding'])\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "audio file=\"audio/2830-3980-0043.wav\"    expected text=\"experience proves this\"\n",
            "azure-batch-stt: \"Experience proves this.\"\n",
            "\n",
            "audio file=\"audio/4507-16021-0012.wav\"    expected text=\"why should one halt on the way\"\n",
            "azure-batch-stt: \"Whi should one halt on the way.\"\n",
            "\n",
            "audio file=\"audio/8455-210777-0068.wav\"    expected text=\"your power is sufficient i said\"\n",
            "azure-batch-stt: \"Your power is sufficient I said.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXd7OC7plbAu"
      },
      "source": [
        "## Streaming API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzfBW4kczY9l",
        "outputId": "8694efbf-5886-4359-ec6c-5b6f3c970372",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "import time\n",
        "import azure.cognitiveservices.speech as speechsdk\n",
        "\n",
        "def azure_streaming_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    speech_config = speechsdk.SpeechConfig(\n",
        "        subscription=AZURE_SPEECH_KEY,\n",
        "        region=AZURE_SERVICE_REGION\n",
        "    )\n",
        "    stream = speechsdk.audio.PushAudioInputStream()\n",
        "    audio_config = speechsdk.audio.AudioConfig(stream=stream)\n",
        "    speech_recognizer = speechsdk.SpeechRecognizer(\n",
        "        speech_config=speech_config,\n",
        "        audio_config=audio_config\n",
        "    )\n",
        "\n",
        "    # Connect callbacks to the events fired by the speech recognizer\n",
        "    speech_recognizer.recognizing.connect(lambda evt: print('interim text: \"{}\"'.format(evt.result.text)))\n",
        "    speech_recognizer.recognized.connect(lambda evt:  print('azure-streaming-stt: \"{}\"'.format(evt.result.text)))\n",
        "\n",
        "    # start continuous speech recognition\n",
        "    speech_recognizer.start_continuous_recognition()\n",
        "\n",
        "    # push buffer chunks to stream\n",
        "    buffer, rate = read_wav_file(filename)\n",
        "    audio_generator = simulate_stream(buffer)\n",
        "    for chunk in audio_generator:\n",
        "      stream.write(chunk)\n",
        "      time.sleep(0.1)  # to give callback a chance against this fast loop\n",
        "\n",
        "    # stop continuous speech recognition\n",
        "    stream.close()\n",
        "    time.sleep(0.5)  # give chance to VAD to kick in\n",
        "    speech_recognizer.stop_continuous_recognition()\n",
        "    time.sleep(0.5)  # Let all callback run\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\n",
        "    azure_streaming_stt(t['filename'], t['lang'], t['encoding'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "audio file=\"audio/2830-3980-0043.wav\"    expected text=\"experience proves this\"\n",
            "interim text: \"experience\"\n",
            "interim text: \"experienced\"\n",
            "interim text: \"experience\"\n",
            "interim text: \"experience proves\"\n",
            "interim text: \"experience proves this\"\n",
            "azure-streaming-stt: \"Experience proves this.\"\n",
            "\n",
            "audio file=\"audio/4507-16021-0012.wav\"    expected text=\"why should one halt on the way\"\n",
            "interim text: \"huaisheng\"\n",
            "interim text: \"white\"\n",
            "interim text: \"whi should\"\n",
            "interim text: \"whi should one\"\n",
            "interim text: \"whi should one halt\"\n",
            "interim text: \"whi should one halt on\"\n",
            "interim text: \"whi should one halt on the\"\n",
            "interim text: \"whi should one halt on the way\"\n",
            "azure-streaming-stt: \"Whi should one halt on the way.\"\n",
            "\n",
            "audio file=\"audio/8455-210777-0068.wav\"    expected text=\"your power is sufficient i said\"\n",
            "interim text: \"you're\"\n",
            "interim text: \"your\"\n",
            "interim text: \"your power\"\n",
            "interim text: \"your\"\n",
            "interim text: \"your power is\"\n",
            "interim text: \"your power is sufficient\"\n",
            "interim text: \"your power is sufficient i\"\n",
            "interim text: \"your power is sufficient i said\"\n",
            "azure-streaming-stt: \"Your power is sufficient I said.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ASpAymRMzOz"
      },
      "source": [
        "---\n",
        "\n",
        "# IBM Watson\n",
        "\n",
        "For IBM [Watson Speech to Text](https://www.ibm.com/in-en/cloud/watson-speech-to-text) is ASR service with .NET, Go, JavaScript, [Python](https://cloud.ibm.com/apidocs/speech-to-text/speech-to-text?code=python), Ruby, Swift and Unity API libraries, as well as REST endpoints.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atuGghM2RxWd"
      },
      "source": [
        "## Setup\n",
        "\n",
        "1. **Install IBM Watson package**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG5jW68yRWGk",
        "outputId": "b3beae11-4775-430c-8f5b-57ad5809bd93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "!pip install ibm-watson"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ibm-watson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/f4/7e256026ee22c75a630c6de53eb45b6fef4840ac6728b80a92dd2e523a1a/ibm-watson-4.2.1.tar.gz (348kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from ibm-watson) (2.21.0)\n",
            "Requirement already satisfied: python_dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from ibm-watson) (2.6.1)\n",
            "Collecting websocket-client==0.48.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/a1/72ef9aa26cfe1a75cee09fc1957e4723add9de098c15719416a1ee89386b/websocket_client-0.48.0-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 43.6MB/s \n",
            "\u001b[?25hCollecting ibm_cloud_sdk_core==1.5.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/f6/10d5271c807d73d236e6ae07b68035fed78b28b5ab836704d34097af3986/ibm-cloud-sdk-core-1.5.1.tar.gz\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.0->ibm-watson) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.0->ibm-watson) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.0->ibm-watson) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.0->ibm-watson) (2019.11.28)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python_dateutil>=2.5.3->ibm-watson) (1.12.0)\n",
            "Collecting PyJWT>=1.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/87/8b/6a9f14b5f781697e51259d81657e6048fd31a113229cf346880bb7545565/PyJWT-1.7.1-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: ibm-watson, ibm-cloud-sdk-core\n",
            "  Building wheel for ibm-watson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-watson: filename=ibm_watson-4.2.1-cp36-none-any.whl size=343298 sha256=3fcdea1185ceb522ed5f080ad4d66048d9286cd28e8d9bc86094b08a84cb6211\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/4d/6e/ae352b7c7acdddf073aeb06617fbfeefaea9fcb6d7ae98800b\n",
            "  Building wheel for ibm-cloud-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cloud-sdk-core: filename=ibm_cloud_sdk_core-1.5.1-cp36-none-any.whl size=44492 sha256=8fbd5fdfa4ca15217877ee44671387c23ce61f390cd15d8006200d502d56dc63\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/42/50/f96888116b329578304f9dda4693cef6f3e76e18272d22cb6c\n",
            "Successfully built ibm-watson ibm-cloud-sdk-core\n",
            "Installing collected packages: websocket-client, PyJWT, ibm-cloud-sdk-core, ibm-watson\n",
            "Successfully installed PyJWT-1.7.1 ibm-cloud-sdk-core-1.5.1 ibm-watson-4.2.1 websocket-client-0.48.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bntnwqJ3Q99Z"
      },
      "source": [
        "2. **Set service credentials**\n",
        "\n",
        "You will need to [sign up/in](https://cloud.ibm.com/docs/services/text-to-speech?topic=text-to-speech-gettingStarted), and get API key credential and service URL, and fill it below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdl6Y7MJPtoT"
      },
      "source": [
        "WATSON_API_KEY = 'YOUR WATSON API KEY'\n",
        "WATSON_STT_URL = 'YOUR WATSON SERVICE URL'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jqxI2XrRmKz"
      },
      "source": [
        "## Batch API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFWX40PYRogi",
        "outputId": "6feb0b03-8ca4-485e-bf25-5042340f8ed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "import os\n",
        "\n",
        "from ibm_watson import SpeechToTextV1\n",
        "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
        "\n",
        "def watson_batch_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    authenticator = IAMAuthenticator(WATSON_API_KEY)\n",
        "    speech_to_text = SpeechToTextV1(authenticator=authenticator)\n",
        "    speech_to_text.set_service_url(WATSON_STT_URL)\n",
        "\n",
        "    with open(filename, 'rb') as audio_file:\n",
        "        response = speech_to_text.recognize(\n",
        "            audio=audio_file,\n",
        "            content_type='audio/{}'.format(os.path.splitext(filename)[1][1:]),\n",
        "            model=lang + '_BroadbandModel',\n",
        "            max_alternatives=3,\n",
        "        ).get_result()\n",
        "\n",
        "    return response['results'][0]['alternatives'][0]['transcript']\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\n",
        "    print('watson-batch-stt: \"{}\"'.format(\n",
        "        watson_batch_stt(t['filename'], t['lang'], t['encoding'])\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "audio file=\"audio/2830-3980-0043.wav\"    expected text=\"experience proves this\"\n",
            "watson-batch-stt: \"experience proves this \"\n",
            "\n",
            "audio file=\"audio/4507-16021-0012.wav\"    expected text=\"why should one halt on the way\"\n",
            "watson-batch-stt: \"why should one hold on the way \"\n",
            "\n",
            "audio file=\"audio/8455-210777-0068.wav\"    expected text=\"your power is sufficient i said\"\n",
            "watson-batch-stt: \"your power is sufficient I set \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOsBdku-RpB-"
      },
      "source": [
        "## Streaming API\n",
        "\n",
        "Streaming API works over websocket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwb0uZjPR0rX",
        "outputId": "97e95e3f-053e-46b7-c459-13697a6eb872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        }
      },
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "from queue import Queue\n",
        "from threading import Thread\n",
        "import time\n",
        "\n",
        "from ibm_watson import SpeechToTextV1\n",
        "from ibm_watson.websocket import RecognizeCallback, AudioSource\n",
        "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
        "\n",
        "# Watson websocket prints justs too many debug logs, so disable it\n",
        "logging.disable(logging.CRITICAL)\n",
        "\n",
        "# Chunk and buffer size\n",
        "CHUNK_SIZE = 4096\n",
        "BUFFER_MAX_ELEMENT = 10\n",
        "\n",
        "# A callback class to process various streaming STT events\n",
        "class MyRecognizeCallback(RecognizeCallback):\n",
        "    def __init__(self):\n",
        "        RecognizeCallback.__init__(self)\n",
        "        self.transcript = None\n",
        "\n",
        "    def on_transcription(self, transcript):\n",
        "        # print('transcript: {}'.format(transcript))\n",
        "        pass\n",
        "\n",
        "    def on_connected(self):\n",
        "        # print('Connection was successful')\n",
        "        pass\n",
        "\n",
        "    def on_error(self, error):\n",
        "        # print('Error received: {}'.format(error))\n",
        "        pass\n",
        "\n",
        "    def on_inactivity_timeout(self, error):\n",
        "        # print('Inactivity timeout: {}'.format(error))\n",
        "        pass\n",
        "\n",
        "    def on_listening(self):\n",
        "        # print('Service is listening')\n",
        "        pass\n",
        "\n",
        "    def on_hypothesis(self, hypothesis):\n",
        "        # print('hypothesis: {}'.format(hypothesis))\n",
        "        pass\n",
        "\n",
        "    def on_data(self, data):\n",
        "        self.transcript = data['results'][0]['alternatives'][0]['transcript']\n",
        "        print('{0}final: {1}'.format(\n",
        "            '' if data['results'][0]['final'] else 'not ',\n",
        "            self.transcript\n",
        "        ))\n",
        "\n",
        "    def on_close(self):\n",
        "        # print(\"Connection closed\")\n",
        "        pass\n",
        "\n",
        "def watson_streaming_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    authenticator = IAMAuthenticator(WATSON_API_KEY)\n",
        "    speech_to_text = SpeechToTextV1(authenticator=authenticator)\n",
        "    speech_to_text.set_service_url(WATSON_STT_URL)\n",
        "\n",
        "    # Make watson audio source fed by a buffer queue\n",
        "    buffer_queue = Queue(maxsize=BUFFER_MAX_ELEMENT)\n",
        "    audio_source = AudioSource(buffer_queue, True, True)\n",
        "\n",
        "    # Callback object\n",
        "    mycallback = MyRecognizeCallback()\n",
        "\n",
        "    # Read the file\n",
        "    buffer, rate = read_wav_file(filename)\n",
        "\n",
        "    # Start Speech-to-Text recognition thread\n",
        "    stt_stream_thread = Thread(\n",
        "        target=speech_to_text.recognize_using_websocket,\n",
        "        kwargs={\n",
        "            'audio': audio_source,\n",
        "            'content_type': 'audio/l16; rate={}'.format(rate),\n",
        "            'recognize_callback': mycallback,\n",
        "            'interim_results': True\n",
        "        }\n",
        "    )\n",
        "    stt_stream_thread.start()\n",
        "\n",
        "    # Simulation audio stream by breaking file into chunks and filling buffer queue\n",
        "    audio_generator = simulate_stream(buffer, CHUNK_SIZE)\n",
        "    for chunk in audio_generator:\n",
        "        buffer_queue.put(chunk)\n",
        "        time.sleep(0.5)  # give a chance to callback\n",
        "\n",
        "    # Close the audio feed and wait for STTT thread to complete\n",
        "    audio_source.completed_recording()\n",
        "    stt_stream_thread.join()\n",
        "\n",
        "    # send final result\n",
        "    return mycallback.transcript\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\n",
        "    print('watson-cloud-streaming-stt: \"{}\"'.format(\n",
        "        watson_streaming_stt(t['filename'], t['lang'], t['encoding'])\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "audio file=\"audio/2830-3980-0043.wav\"    expected text=\"experience proves this\"\n",
            "not final: X. \n",
            "not final: experts \n",
            "not final: experience \n",
            "not final: experienced \n",
            "not final: experience prove \n",
            "not final: experience proves \n",
            "not final: experience proves that \n",
            "not final: experience proves this \n",
            "final: experience proves this \n",
            "watson-cloud-streaming-stt: \"experience proves this \"\n",
            "\n",
            "audio file=\"audio/4507-16021-0012.wav\"    expected text=\"why should one halt on the way\"\n",
            "not final: why \n",
            "not final: what \n",
            "not final: why should \n",
            "not final: why should we \n",
            "not final: why should one \n",
            "not final: why should one whole \n",
            "not final: why should one hold \n",
            "not final: why should one hold on \n",
            "not final: why should one hold on the \n",
            "not final: why should one hold on the way \n",
            "final: why should one hold on the way \n",
            "watson-cloud-streaming-stt: \"why should one hold on the way \"\n",
            "\n",
            "audio file=\"audio/8455-210777-0068.wav\"    expected text=\"your power is sufficient i said\"\n",
            "not final: your \n",
            "not final: your power \n",
            "not final: your power is \n",
            "not final: your power is the \n",
            "not final: your power is sufficient \n",
            "not final: your power is sufficient I \n",
            "not final: your power is sufficient I saw \n",
            "not final: your power is sufficient I said \n",
            "not final: your power is sufficient I set \n",
            "final: your power is sufficient I set \n",
            "watson-cloud-streaming-stt: \"your power is sufficient I set \"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7W8nsP45IUx"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# CMU Sphinx\n",
        "\n",
        "[CMUSphinx](https://cmusphinx.github.io/) is has been around for quite some time, and has been adapting to advancements in ASR technologies. [PocketSphinx](https://github.com/cmusphinx/pocketsphinx-python) is speech-to-text decoder software package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSdcTyoTl3XL"
      },
      "source": [
        "## Setup\n",
        "\n",
        "1. **Install swig**\n",
        "\n",
        "For macOS:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMWqogsSSk2H"
      },
      "source": [
        "!brew install swig\n",
        "!swig -version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn_LqRxjSoMT"
      },
      "source": [
        "For Linux:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID2AUZX4SqkU",
        "outputId": "01770ecb-4b8d-4047-c1a2-2f1490c2f74b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        }
      },
      "source": [
        "!apt-get install -y swig libpulse-dev\n",
        "!swig -version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libpulse-mainloop-glib0 swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  libpulse-dev libpulse-mainloop-glib0 swig swig3.0\n",
            "0 upgraded, 4 newly installed, 0 to remove and 7 not upgraded.\n",
            "Need to get 1,204 kB of archives.\n",
            "After this operation, 6,538 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpulse-mainloop-glib0 amd64 1:11.1-1ubuntu7.4 [22.1 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpulse-dev amd64 1:11.1-1ubuntu7.4 [81.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 1,204 kB in 1s (1,336 kB/s)\n",
            "Selecting previously unselected package libpulse-mainloop-glib0:amd64.\n",
            "(Reading database ... 145674 files and directories currently installed.)\n",
            "Preparing to unpack .../libpulse-mainloop-glib0_1%3a11.1-1ubuntu7.4_amd64.deb ...\n",
            "Unpacking libpulse-mainloop-glib0:amd64 (1:11.1-1ubuntu7.4) ...\n",
            "Selecting previously unselected package libpulse-dev:amd64.\n",
            "Preparing to unpack .../libpulse-dev_1%3a11.1-1ubuntu7.4_amd64.deb ...\n",
            "Unpacking libpulse-dev:amd64 (1:11.1-1ubuntu7.4) ...\n",
            "Selecting previously unselected package swig3.0.\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up libpulse-mainloop-glib0:amd64 (1:11.1-1ubuntu7.4) ...\n",
            "Setting up libpulse-dev:amd64 (1:11.1-1ubuntu7.4) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "\n",
            "SWIG Version 3.0.12\n",
            "\n",
            "Compiled with g++ [x86_64-pc-linux-gnu]\n",
            "\n",
            "Configured options: +pcre\n",
            "\n",
            "Please see http://www.swig.org for reporting bugs and further information\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZlvnjsfSu-3"
      },
      "source": [
        "2. **Install poocketsphinx using pip**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzOkKfgKS789",
        "outputId": "116c7a60-cf24-4ba9-f3ac-ab09a5da0145",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "!pip3 install pocketsphinx\n",
        "!pip3 list | grep pocketsphinx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pocketsphinx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/4a/adea55f189a81aed88efa0b0e1d25628e5ed22622ab9174bf696dd4f9474/pocketsphinx-0.1.15.tar.gz (29.1MB)\n",
            "\u001b[K     |████████████████████████████████| 29.1MB 102kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pocketsphinx\n",
            "  Building wheel for pocketsphinx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pocketsphinx: filename=pocketsphinx-0.1.15-cp36-cp36m-linux_x86_64.whl size=30126870 sha256=d111bc1a768251e9b8b4bea71f05b498955eda209f5d5650f7e68cc336bb5075\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/fd/52/2f62c9a0036940cc0c89e58ee0b9d00fcf78243aeaf416265f\n",
            "Successfully built pocketsphinx\n",
            "Installing collected packages: pocketsphinx\n",
            "Successfully installed pocketsphinx-0.1.15\n",
            "pocketsphinx                   0.1.15     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weYD8oA-S-vu"
      },
      "source": [
        "## Create Decoder object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEpNoVUiTK4k"
      },
      "source": [
        "import pocketsphinx\n",
        "import os\n",
        "\n",
        "MODELDIR = os.path.join(os.path.dirname(pocketsphinx.__file__), 'model')\n",
        "\n",
        "config = pocketsphinx.Decoder.default_config()\n",
        "config.set_string('-hmm', os.path.join(MODELDIR, 'en-us'))\n",
        "config.set_string('-lm', os.path.join(MODELDIR, 'en-us.lm.bin'))\n",
        "config.set_string('-dict', os.path.join(MODELDIR, 'cmudict-en-us.dict'))\n",
        "\n",
        "decoder = pocketsphinx.Decoder(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6s9CCA9WvIZ"
      },
      "source": [
        "## Batch API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klStePTBWxO7",
        "outputId": "c06996f3-5da9-4b1a-a5bc-a18c584da3e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "def sphinx_batch_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    buffer, rate = read_wav_file(filename)\n",
        "    decoder.start_utt()\n",
        "    decoder.process_raw(buffer, False, False)\n",
        "    decoder.end_utt()\n",
        "    hypothesis = decoder.hyp()\n",
        "    return hypothesis.hypstr\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\n",
        "    print('sphinx-batch-stt: \"{}\"'.format(\n",
        "        sphinx_batch_stt(t['filename'], t['lang'], t['encoding'])\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "audio file=\"audio/2830-3980-0043.wav\"    expected text=\"experience proves this\"\n",
            "sphinx-batch-stt: \"experience proves this\"\n",
            "\n",
            "audio file=\"audio/4507-16021-0012.wav\"    expected text=\"why should one halt on the way\"\n",
            "sphinx-batch-stt: \"why should one hold on the way\"\n",
            "\n",
            "audio file=\"audio/8455-210777-0068.wav\"    expected text=\"your power is sufficient i said\"\n",
            "sphinx-batch-stt: \"your paris sufficient i said\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQJ82tkTTyX3"
      },
      "source": [
        "## Streaming API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGfmRd6qTzq9",
        "outputId": "65acfc30-d65e-432b-8abe-ed101ae4ee00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "def sphinx_streaming_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    buffer, rate = read_wav_file(filename)\n",
        "    audio_generator = simulate_stream(buffer)\n",
        "\n",
        "    decoder.start_utt()\n",
        "    for chunk in audio_generator:\n",
        "        decoder.process_raw(chunk, False, False)\n",
        "    decoder.end_utt()\n",
        "\n",
        "    hypothesis = decoder.hyp()\n",
        "    return hypothesis.hypstr\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\n",
        "    print('sphinx-streaming-stt: \"{}\"'.format(\n",
        "        sphinx_streaming_stt(t['filename'], t['lang'], t['encoding'])\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "audio file=\"audio/2830-3980-0043.wav\"    expected text=\"experience proves this\"\n",
            "sphinx-streaming-stt: \"experience proves this\"\n",
            "\n",
            "audio file=\"audio/4507-16021-0012.wav\"    expected text=\"why should one halt on the way\"\n",
            "sphinx-streaming-stt: \"why should one hold on the way\"\n",
            "\n",
            "audio file=\"audio/8455-210777-0068.wav\"    expected text=\"your power is sufficient i said\"\n",
            "sphinx-streaming-stt: \"your paris sufficient i said\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awZEgZKG5cWg"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Mozilla DeepSpeech\n",
        "\n",
        "Mozilla released [DeepSpeech 0.6](https://hacks.mozilla.org/2019/12/deepspeech-0-6-mozillas-speech-to-text-engine/) software package in December 2019 with [APIs](https://github.com/mozilla/DeepSpeech/releases/tag/v0.6.0) in C, Java, .NET, [Python](https://deepspeech.readthedocs.io/en/v0.6.0/Python-API.html), and JavaScript, including support for TensorFlow Lite models for use on edge devices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilmp9i-ql7V1"
      },
      "source": [
        "## Setup\n",
        "\n",
        "1. **Install DeepSpeech**\n",
        "\n",
        "You can install DeepSpeech with pip (make it deepspeech-gpu==0.6.0 if you are using GPU in colab runtime)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbWPbs_27f3Y",
        "outputId": "583f2b3c-cdea-4027-b859-13118fc4b538",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "!pip install deepspeech==0.6.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deepspeech==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/f4/1ef0397097e8a8bbb7e24caabecbdb226b4e027e5018e9353ef65af14672/deepspeech-0.6.0-cp36-cp36m-manylinux1_x86_64.whl (9.6MB)\n",
            "\u001b[K     |████████████████████████████████| 9.6MB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from deepspeech==0.6.0) (1.17.5)\n",
            "Installing collected packages: deepspeech\n",
            "Successfully installed deepspeech-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIe7haLO7yo4"
      },
      "source": [
        "2. **Download and unzip models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT-n1jLj8Ff4",
        "outputId": "eb58aab5-aafe-4d3c-97dc-58ad4fd7e6b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "!curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.6.0/deepspeech-0.6.0-models.tar.gz\n",
        "!tar -xvzf deepspeech-0.6.0-models.tar.gz\n",
        "!ls -l ./deepspeech-0.6.0-models/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   620    0   620    0     0   2857      0 --:--:-- --:--:-- --:--:--  2857\n",
            "100 1172M  100 1172M    0     0  48.9M      0  0:00:23  0:00:23 --:--:-- 56.8M\n",
            "deepspeech-0.6.0-models/\n",
            "deepspeech-0.6.0-models/lm.binary\n",
            "deepspeech-0.6.0-models/output_graph.pbmm\n",
            "deepspeech-0.6.0-models/output_graph.pb\n",
            "deepspeech-0.6.0-models/trie\n",
            "deepspeech-0.6.0-models/output_graph.tflite\n",
            "total 1350664\n",
            "-rw-r--r-- 1 501 staff 945699324 Dec  3 06:51 lm.binary\n",
            "-rw-r--r-- 1 501 staff 188914896 Dec  3 09:03 output_graph.pb\n",
            "-rw-r--r-- 1 501 staff 188915850 Dec  3 09:49 output_graph.pbmm\n",
            "-rw-r--r-- 1 501 staff  47335752 Dec  3 09:05 output_graph.tflite\n",
            "-rw-r--r-- 1 501 staff  12200736 Dec  3 06:51 trie\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGGaM4wp8Ykp"
      },
      "source": [
        "3. **Test that it all works**\n",
        "\n",
        "Examine the output of the last three commands, and you will see results *“experience proof less”*, *“why should one halt on the way”*, and *“your power is sufficient i said”* respectively. You are all set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pPnZssj8fPY",
        "outputId": "5ebaeec2-f484-4047-9766-026a3f53d730",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "!deepspeech --model deepspeech-0.6.0-models/output_graph.pb --lm deepspeech-0.6.0-models/lm.binary --trie ./deepspeech-0.6.0-models/trie --audio ./audio/2830-3980-0043.wav"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model from file deepspeech-0.6.0-models/output_graph.pb\n",
            "TensorFlow: v1.14.0-21-ge77504a\n",
            "DeepSpeech: v0.6.0-0-g6d43e21\n",
            "Warning: reading entire model file into memory. Transform model file into an mmapped graph to reduce heap usage.\n",
            "2020-01-30 00:27:46.675441: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "Loaded model in 0.13s.\n",
            "Loading language model from files deepspeech-0.6.0-models/lm.binary ./deepspeech-0.6.0-models/trie\n",
            "Loaded language model in 0.000221s.\n",
            "Running inference.\n",
            "experience proof less\n",
            "Inference took 2.418s for 1.975s audio file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvxm5RE68zu4",
        "outputId": "84c877c7-d1fd-4bd9-ae96-56f63bf37dba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "!deepspeech --model deepspeech-0.6.0-models/output_graph.pb --lm deepspeech-0.6.0-models/lm.binary --trie ./deepspeech-0.6.0-models/trie --audio ./audio/4507-16021-0012.wav"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model from file deepspeech-0.6.0-models/output_graph.pb\n",
            "TensorFlow: v1.14.0-21-ge77504a\n",
            "DeepSpeech: v0.6.0-0-g6d43e21\n",
            "Warning: reading entire model file into memory. Transform model file into an mmapped graph to reduce heap usage.\n",
            "2020-01-30 00:27:53.427469: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "Loaded model in 0.131s.\n",
            "Loading language model from files deepspeech-0.6.0-models/lm.binary ./deepspeech-0.6.0-models/trie\n",
            "Loaded language model in 0.000188s.\n",
            "Running inference.\n",
            "why should one halt on the way\n",
            "Inference took 2.941s for 2.735s audio file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Hq_tEFQ8254",
        "outputId": "7f4a4720-72da-442a-ea4d-d7f08a66ec0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "!deepspeech --model deepspeech-0.6.0-models/output_graph.pb --lm deepspeech-0.6.0-models/lm.binary --trie ./deepspeech-0.6.0-models/trie --audio ./audio/8455-210777-0068.wav"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model from file deepspeech-0.6.0-models/output_graph.pb\n",
            "TensorFlow: v1.14.0-21-ge77504a\n",
            "DeepSpeech: v0.6.0-0-g6d43e21\n",
            "Warning: reading entire model file into memory. Transform model file into an mmapped graph to reduce heap usage.\n",
            "2020-01-30 00:28:00.365841: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "Loaded model in 0.129s.\n",
            "Loading language model from files deepspeech-0.6.0-models/lm.binary ./deepspeech-0.6.0-models/trie\n",
            "Loaded language model in 0.000228s.\n",
            "Running inference.\n",
            "your power is sufficient i said\n",
            "Inference took 2.839s for 2.590s audio file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTcABJ2c9CRa"
      },
      "source": [
        "## Create model object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU41WTEr9G-X",
        "outputId": "8c4f73ad-f61f-4467-a3fa-23ef5375de74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import deepspeech\n",
        "\n",
        "model_file_path = 'deepspeech-0.6.0-models/output_graph.pbmm'\n",
        "beam_width = 500\n",
        "model = deepspeech.Model(model_file_path, beam_width)\n",
        "\n",
        "# Add language model for better accuracy\n",
        "lm_file_path = 'deepspeech-0.6.0-models/lm.binary'\n",
        "trie_file_path = 'deepspeech-0.6.0-models/trie'\n",
        "lm_alpha = 0.75\n",
        "lm_beta = 1.85\n",
        "model.enableDecoderWithLM(lm_file_path, trie_file_path, lm_alpha, lm_beta)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB4wl_9P9ilW"
      },
      "source": [
        "## Batch API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTaKt_rm9wY_",
        "outputId": "8bc1dc02-3c8b-4a66-ddb4-b61f362167e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def deepspeech_batch_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    buffer, rate = read_wav_file(filename)\n",
        "    data16 = np.frombuffer(buffer, dtype=np.int16)\n",
        "    return model.stt(data16)\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\n",
        "    print('deepspeech-batch-stt: \"{}\"'.format(\n",
        "        deepspeech_batch_stt(t['filename'], t['lang'], t['encoding'])\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "audio file=\"audio/2830-3980-0043.wav\"    expected text=\"experience proves this\"\n",
            "deepspeech-batch-stt: \"experience proof less\"\n",
            "\n",
            "audio file=\"audio/4507-16021-0012.wav\"    expected text=\"why should one halt on the way\"\n",
            "deepspeech-batch-stt: \"why should one halt on the way\"\n",
            "\n",
            "audio file=\"audio/8455-210777-0068.wav\"    expected text=\"your power is sufficient i said\"\n",
            "deepspeech-batch-stt: \"your power is sufficient i said\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v3jT8NR-qGb"
      },
      "source": [
        "## Streaming API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU7lHQ2A-svH",
        "outputId": "8fc02288-a1a9-4709-ef25-bd42c4c99bf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "def deepspeech_streaming_stt(filename: str, lang: str, encoding: str) -> str:\n",
        "    buffer, rate = read_wav_file(filename)\n",
        "    audio_generator = simulate_stream(buffer)\n",
        "\n",
        "    # Create stream\n",
        "    context = model.createStream()\n",
        "\n",
        "    text = ''\n",
        "    for chunk in audio_generator:\n",
        "        data16 = np.frombuffer(chunk, dtype=np.int16)\n",
        "        # feed stream of chunks\n",
        "        model.feedAudioContent(context, data16)\n",
        "        interim_text = model.intermediateDecode(context)\n",
        "        if interim_text != text:\n",
        "            text = interim_text\n",
        "            print('inetrim text: {}'.format(text))\n",
        "\n",
        "    # get final resut and close stream\n",
        "    text = model.finishStream(context)\n",
        "    return text\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(t['filename'], t['text']))\n",
        "    print('deepspeech-streaming-stt: \"{}\"'.format(\n",
        "        deepspeech_streaming_stt(t['filename'], t['lang'], t['encoding'])\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "audio file=\"audio/2830-3980-0043.wav\"    expected text=\"experience proves this\"\n",
            "inetrim text: i\n",
            "inetrim text: e\n",
            "inetrim text: experi en\n",
            "inetrim text: experience pro\n",
            "inetrim text: experience proof les\n",
            "deepspeech-streaming-stt: \"experience proof less\"\n",
            "\n",
            "audio file=\"audio/4507-16021-0012.wav\"    expected text=\"why should one halt on the way\"\n",
            "inetrim text: i\n",
            "inetrim text: why shou\n",
            "inetrim text: why should one\n",
            "inetrim text: why should one haul\n",
            "inetrim text: why should one halt \n",
            "inetrim text: why should one halt on the \n",
            "deepspeech-streaming-stt: \"why should one halt on the way\"\n",
            "\n",
            "audio file=\"audio/8455-210777-0068.wav\"    expected text=\"your power is sufficient i said\"\n",
            "inetrim text: i\n",
            "inetrim text: your p\n",
            "inetrim text: your power is\n",
            "inetrim text: your power is suffi\n",
            "inetrim text: your power is sufficient i\n",
            "inetrim text: your power is sufficient i said\n",
            "deepspeech-streaming-stt: \"your power is sufficient i said\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aqlb4wEcdOx"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# SpeechRecognition Package\n",
        "\n",
        "The [SpeechRecognition](https://pypi.org/project/SpeechRecognition/) package provide a nice abstraction over several solutions. In this notebook we explore using CMU Sphinx (i.e. model running locally on the machine), and Google (i.e. service accessed over the network/cloud), but both through SpeechRecognition package APIs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpxAVH5OmPtn"
      },
      "source": [
        "## Setup\n",
        "\n",
        "We need to install SpeechRecognition and pocketsphinx python packages, and download some files to test these APIs.\n",
        "\n",
        "1. **Install SpeechRecognition py package**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ0rokUuby2i",
        "outputId": "c0d99348-92e9-49f7-edf0-20493983a1e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!pip3 install SpeechRecognition"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting SpeechRecognition\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/e1/7f5678cd94ec1234269d23756dbdaa4c8cfaed973412f88ae8adf7893a50/SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8MB 92kB/s \n",
            "\u001b[?25hInstalling collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBjNf3GoTU1l"
      },
      "source": [
        "Pocketsphinx has already been installed in earlier sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piIB_P7CXey4"
      },
      "source": [
        "## Batch API\n",
        "\n",
        "SpeechRecognition has only batch API. First step to create an audio record, eithher from a file or from mic, and the second step is to call `recognize_<speech engine name>` function. It currently has APIs for [CMU Sphinx, Google, Microsoft, IBM, Houndify, and Wit](https://github.com/Uberi/speech_recognition)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aia5lFgb-vV",
        "outputId": "bdf84ea8-98f0-43b9-e5f5-305c9745795e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "import speech_recognition as sr\n",
        "from enum import Enum, unique\n",
        "\n",
        "@unique\n",
        "class ASREngine(Enum):\n",
        "    sphinx = 0\n",
        "    google = 1\n",
        "\n",
        "def speech_to_text(filename: str, engine: ASREngine, language: str, show_all: bool = False) -> str:\n",
        "    r = sr.Recognizer()\n",
        "\n",
        "    with sr.AudioFile(filename) as source:\n",
        "        audio = r.record(source)\n",
        "\n",
        "    asr_functions = {\n",
        "        ASREngine.sphinx: r.recognize_sphinx,\n",
        "        ASREngine.google: r.recognize_google,\n",
        "    }\n",
        "\n",
        "    response = asr_functions[engine](audio, language=language, show_all=show_all)\n",
        "    return response\n",
        "\n",
        "# Run tests\n",
        "for t in TESTCASES:\n",
        "    filename = t['filename']\n",
        "    text = t['text']\n",
        "    lang = t['lang']\n",
        "\n",
        "    print('\\naudio file=\"{0}\"    expected text=\"{1}\"'.format(filename, text))\n",
        "    for asr_engine in ASREngine:\n",
        "        try:\n",
        "            response = speech_to_text(filename, asr_engine, language=lang)\n",
        "            print('{0}: \"{1}\"'.format(asr_engine.name, response))\n",
        "        except sr.UnknownValueError:\n",
        "            print('{0} could not understand audio'.format(asr_engine.name))\n",
        "        except sr.RequestError as e:\n",
        "            print('{0} error: {0}'.format(asr_engine.name, e))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "audio file=\"audio/2830-3980-0043.wav\"    expected text=\"experience proves this\"\n",
            "sphinx: \"experience proves that\"\n",
            "google: \"experience proves this\"\n",
            "\n",
            "audio file=\"audio/4507-16021-0012.wav\"    expected text=\"why should one halt on the way\"\n",
            "sphinx: \"why should one hold on the way\"\n",
            "google: \"why should one halt on the way\"\n",
            "\n",
            "audio file=\"audio/8455-210777-0068.wav\"    expected text=\"your power is sufficient i said\"\n",
            "sphinx: \"your paris official said\"\n",
            "google: \"your power is sufficient I said\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66lLoLCaL_nE"
      },
      "source": [
        "### API for other providers\n",
        "\n",
        "For other speech recognition providers, you will need to create API credentials, which you have to pass to `recognize_<speech engine name>` function, you can checkout [this example](https://github.com/Uberi/speech_recognition/blob/master/examples/audio_transcribe.py).\n",
        "\n",
        "It also has a nice abstraction for Microphone, implemented over PyAudio/PortAudio. Here is an example to capture input from mic in [batch](https://github.com/Uberi/speech_recognition/blob/master/examples/microphone_recognition.py) and continously in [background](https://github.com/Uberi/speech_recognition/blob/master/examples/background_listening.py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTfKgcgF0uzz"
      },
      "source": [
        "---\n",
        "\n",
        "# Summary\n",
        "\n",
        "This note covers various available speech recognition:\n",
        "\n",
        "- services: Google, Azure, Watson\n",
        "- software: CMU Sphinx, Mozilla DeepSpeech\n",
        "\n",
        "All of these have two kind of Speech-to-Text APIs:\n",
        "\n",
        "- batch: the audio data is fed in one go\n",
        "- streaming: the audio data is fed in chunks (very useful for transcribing microphone input)\n",
        "\n",
        "The Python SpeechRecognition package provides abstraction over several speech recognition services and softwares.\n",
        "\n",
        "I hope to include following in future:\n",
        "\n",
        "- services: [Amazon Transcribe](https://aws.amazon.com/transcribe/), and [Nuance](https://nuancedev.github.io/samples/http/python/)\n",
        "- software: [Kaldi](https://pykaldi.github.io/), and [Facebook wav2letter](https://ai.facebook.com/blog/online-speech-recognition-with-wav2letteranywhere/)\n",
        "\n",
        "<br/>\n",
        "\n",
        "---"
      ]
    }
  ]
}